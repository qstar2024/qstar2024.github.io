{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Asset Allocation\n",
    "10%-vol cross-asset portfolio consists of both traditional and alternative risk premia.\n",
    "\n",
    "quarterly rebalancing, no leverage constraints, no transaction costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org cvxportfolio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import Bounds\n",
    "from scipy.optimize import LinearConstraint\n",
    "from scipy.optimize import NonlinearConstraint\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import LinearOperator\n",
    "import cvxportfolio as cvx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_rp = {\n",
    "    \"ARP_EqLoR\":\"MSQEFLRG Index\", \n",
    "    \"ARP_EqVal\":\"MSQEFVAG Index\", \n",
    "    \"ARP_EqMom\":\"MSQEFMOG Index\", \n",
    "    \"ARP_EqQly\":\"MSQEFQUG Index\", \n",
    "    \"ARP_FXCry\":\"SGIFXC10 Index\", \n",
    "    \"ARP_FXVal\":\"SGIFXM10 Index\", \n",
    "    \"ARP_FIMom\":\"BXIIXTBP Index\", \n",
    "    \"ARP_FIVal\":\"BXIISAGU Index\", \n",
    "    \"ARP_FICry\":\"BXIIEPAG Index\", \n",
    "    \"TRP_Cmdty\":\"BCOM Index\", \n",
    "    \"TRP_EqLrg\":\"RIY Index\", \n",
    "    \"TRP_EqSml\":\"RTY Index\", \n",
    "    \"TRP_EqGEM\":\"MXEF Index\", \n",
    "    \"TRP_BbgAgg\":\"LBUSTRUU Index\", \n",
    "    \"TRP_USHY\":\"LF98TRUU Index\", \n",
    "    \"TRP_CSLL\":\"CSLLLTOT Index\", \n",
    "    \"TRP_Cembi\":\"JCBDCOMP Index\", \n",
    "    \"TRP_Embi\":\"JPEIDIVR Index\", \n",
    "    \"TRP_NexGen\":\"NGEMCOMP Index\", \n",
    "    \"TRP_GbiEm\":\"JGENVUUG Index\", \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "https://stackoverflow.com/questions/68831145/fast-way-to-get-rolling-percentile-ranks\n",
    "\"\"\"\n",
    "def getPercentile(df, lookback=1000):\n",
    "    data = df.to_numpy()\n",
    "    sw = np.lib.stride_tricks.sliding_window_view(data, lookback, axis=0).T\n",
    "    scores_np = (sw <= sw[-1:, ...]).sum(axis=0).T / sw.shape[0]\n",
    "    nan_nparray = np.empty((lookback-1, df.shape[1]))\n",
    "    nan_nparray[:] = np.nan\n",
    "    scores_np_full = np.insert(scores_np, 0, nan_nparray, axis=0)\n",
    "    scores_np_df = pd.DataFrame(scores_np_full, columns=df.columns, index=df.index)\n",
    "    return scores_np_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hierarchical Clustering Asset Allocation\n",
    "Adapted from:\n",
    "    David Bailey & Marcos Lopez de Prado (2013) \"An Open-Source Implementation of the Critical-Line Algorithm for Portfolio Optimization\"\n",
    "    Marcos Lopez de Prado (2016) \"Building diversified portfolios that outperform out of sample\"\n",
    "    Thomas Raffinot (2017) \"Hierarchical Clustering based Asset Allocation\"\n",
    "    Berowne Hlavaty & Robert Smith (2017) \"Post-Modern Portfolio Construction: Examining Recent Innovations in Asset Allocation\"\n",
    "\"\"\"\n",
    "\n",
    "def getHRP(cov, corr):\n",
    "    # Construct a hierarchical portfolio\n",
    "    corr, cov = pd.DataFrame(corr), pd.DataFrame(cov)\n",
    "    dist = correlDist(corr)\n",
    "    link = sch.linkage(dist, 'single')\n",
    "    sortIx = getQuasiDiag(link)\n",
    "    sortIx = corr.index[sortIx].tolist() # recover labels\n",
    "    hrp = getRecBipart(cov, sortIx)\n",
    "    return hrp.sort_index()\n",
    "\n",
    "\n",
    "def getIVP(cov, **kargs):\n",
    "    # Compute the inverse-variance portfolio\n",
    "    ivp = 1. / np.diag(cov)\n",
    "    ivp /= ivp.sum()\n",
    "    return ivp\n",
    "\n",
    "\n",
    "def getClusterVar(cov, cItems):\n",
    "    # Compute variance per cluster\n",
    "    cov_ = cov.loc[cItems,cItems] # matrix slice\n",
    "    w_ = getIVP(cov_).reshape(-1,1)\n",
    "    cVar = np.dot(np.dot(w_.T,cov_),w_)[0,0]\n",
    "    return cVar\n",
    "\n",
    "\n",
    "def getQuasiDiag(link):\n",
    "    # Sort clustered items by distance\n",
    "    link = link.astype(int)\n",
    "    sortIx = pd.Series([link[-1,0], link[-1,1]])\n",
    "    numItems = link[-1,3] # number of original items\n",
    "    while sortIx.max() >= numItems:\n",
    "        sortIx.index = range(0, sortIx.shape[0]*2, 2) # make space\n",
    "        df0 = sortIx[sortIx>=numItems] # find clusters\n",
    "        i = df0.index\n",
    "        j = df0.values - numItems\n",
    "        sortIx[i] = link[j,0] # item 1\n",
    "        df0 = pd.Series(link[j,1], index=i+1)\n",
    "        sortIx = pd.concat([sortIx, df0]) #sortIx = sortIx.append(df0) # item 2\n",
    "        sortIx = sortIx.sort_index() # re-sort\n",
    "        sortIx.index = range(sortIx.shape[0]) # re-index\n",
    "    return sortIx.tolist()\n",
    "\n",
    "\n",
    "def getRecBipart(cov, sortIx):\n",
    "    # Compute HRP alloc\n",
    "    w = pd.Series(1,index=sortIx)\n",
    "    cItems = [sortIx] # initialize all items in one cluster\n",
    "    while len(cItems) > 0:\n",
    "        cItems = [i[j:k] for i in cItems \n",
    "                  for j,k in ((0,len(i)//2), (len(i)//2,len(i))) \n",
    "                  if len(i)>1] # bi-section\n",
    "        # parse in pairs\n",
    "        for i in range(0, len(cItems), 2):\n",
    "            cItems0 = cItems[i] # cluster 1\n",
    "            cItems1 = cItems[i+1] # cluster 2\n",
    "            cVar0 = getClusterVar(cov, cItems0)\n",
    "            cVar1 = getClusterVar(cov, cItems1)\n",
    "            alpha = 1 - cVar0/(cVar0+cVar1)\n",
    "            w[cItems0] *= alpha # weight 1\n",
    "            w[cItems1] *= 1-alpha # weight 2\n",
    "    return w\n",
    "\n",
    "\n",
    "def getHARP(cov, sortIx, xReturns=None, riskaversion=0.5, minWt = 0.001, maxWt = 1.0):\n",
    "    ##### JPM Oct 10 2017, Berowne Hlavaty & Robert Smith #####\n",
    "    # Compute HAP, HRP and HARP asset allocation\n",
    "    # xReturns Asset Returns must be rescaled from zero to 1.0! getxprtnnZeroOne(expReturns)\n",
    "    # riskaversion = Lambda relative importance of risk vs expected returns\n",
    "    # Based on de Prado’s “getRecBipart” function\n",
    "    w=pd.Series(1.0/sortIx.__len__() ,index=sortIx)\n",
    "    wts=pd.DataFrame(data=0,index=sortIx, columns=[0])\n",
    "    cItems=[sortIx] # initialize all items in one cluster\n",
    "    wti=0\n",
    "    while len(cItems)>0:\n",
    "        cItems=[i[j:k] for i in cItems for j,k in ((0,len(i)//2), (len(i)//2,len(i))) if len(i)>1] # bi-section\n",
    "        for i in range(0,len(cItems),2): # parse in pairs i=0\n",
    "            cItems0=cItems[i] # cluster 1\n",
    "            cItems1=cItems[i+1] # cluster 2\n",
    "            cVar0=getClusterVar(cov,cItems0) # Single variance number for portfolio 0\n",
    "            cVar1=getClusterVar(cov,cItems1)\n",
    "            if xReturns is None:\n",
    "                alpha=1-cVar0/(cVar0+cVar1)\n",
    "                w[cItems0]*=alpha # weight 1\n",
    "                w[cItems1]*=1-alpha # weight 2\n",
    "            else:\n",
    "                eVar0 = xReturns[cItems0].values.mean() # Expected Returns of group 1\n",
    "                eVar1 = xReturns[cItems1].values.mean() # Expected Returns of group 2\n",
    "                alpha = 1. - cVar0 / (cVar0+cVar1) # HRPi Relative Variance - HRP 'alpha'\n",
    "                xprtn = 0. + eVar0 / (eVar0+eVar1) # HAPi Expected Return Relative\n",
    "                # High Risk Aversion results in more stock weight from variance vs. returns.\n",
    "                w[cItems0] *= riskaversion*alpha + (xprtn*(1.-riskaversion))# HARPi Wt1\n",
    "                w[cItems1] *= riskaversion*(1.-alpha) + (1.-xprtn)*((1.-riskaversion))#Wt2\n",
    "        wts[wti] = w #store incremental weights for debugging\n",
    "        wti +=1\n",
    "        w[w<minWt] = 0.0\n",
    "        w = w.clip(0.0, maxWt)\n",
    "        w[w<maxWt] = w[w<maxWt] / sum(w) # fix rounding errors\n",
    "    return w\n",
    "\n",
    "\n",
    "def getClusters(corr = pd.DataFrame, method='ward', bPlot=False):\n",
    "    ##### JPM Oct 10 2017, Berowne Hlavaty & Robert Smith #####\n",
    "    # HCP\n",
    "    corr = corr.replace([np.inf, -np.inf], np.nan).fillna(0, inplace=False)\n",
    "    clusters = []\n",
    "    w=pd.Series(1.0,index=corr.index)\n",
    "    Z = sch.linkage(corr, method) # 'ward', 'single', etc...)\n",
    "    # Combine raw leaf nodes with linkage matrix\n",
    "    raw = pd.DataFrame(corr.index, index=corr.index, columns=['cItems0'])\n",
    "    raw['cItems1'] = np.nan\n",
    "    raw['Dist'] = 0\n",
    "    raw['Count'] = 1\n",
    "    link = pd.DataFrame(Z, columns=['cItems0','cItems1','Dist','Count'])\n",
    "    linked = pd.concat([raw, link],axis=0, ignore_index=True)\n",
    "    # Initial Weights\n",
    "    linked['wt']=1.\n",
    "    # Traverse linkage in reverse order\n",
    "    w = linked.__len__()-1\n",
    "    while w>Z.__len__():\n",
    "        inWt = linked.loc[w,'wt']/2.0 # weight split\n",
    "        cItems0, cItems1 = linked.iloc[w,0:2].astype(int)\n",
    "        linked.loc[[cItems0, cItems1],'wt'] *= inWt\n",
    "        w-=1\n",
    "    if bPlot: # Do we need a chart\n",
    "        fig = PlotDendo(Z, labels=linked.loc[0:Z.__len__(),'wt'].values) # corr.index)\n",
    "    return linked.loc[0:Z.__len__(),'wt']\n",
    "\n",
    "\n",
    "def PlotDendo(Z, labels):\n",
    "    ##### JPM Oct 10 2017, Berowne Hlavaty & Robert Smith #####\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title('Hierarchical Clustering Dendrogram on Correlations')\n",
    "    # plt.xlabel('sample index')\n",
    "    # plt.ylabel('distance')\n",
    "    R = sch.dendrogram(\n",
    "        Z,\n",
    "        leaf_rotation=90., # rotates the x axis labels\n",
    "        leaf_font_size=16., # font size for the x axis labels\n",
    "        labels=[str(word*100.0) + '%' for word in labels], # label for plot\n",
    "        color_threshold=None # .5\n",
    "    )\n",
    "    ax = plt.gca()\n",
    "    ax.tick_params(axis='y', labelsize=16)\n",
    "    plt.show()\n",
    "    return R\n",
    "\n",
    "\n",
    "def getxprtnnZeroOne(expReturns=pd.Series):\n",
    "    ##### JPM Oct 10 2017, Berowne Hlavaty & Robert Smith #####\n",
    "    # Rescale expected returns from 0 to 1\n",
    "    if expReturns is None:\n",
    "        pass\n",
    "    else:\n",
    "        idx=expReturns.index\n",
    "        expReturns = zscore(expReturns)\n",
    "        expReturns = expReturns/max(abs(expReturns))/2.0+0.5\n",
    "        expReturns = expReturns / max(abs(expReturns))\n",
    "        # in case when max(abs()) was -ve, then new max will be < 1.0\n",
    "        expReturns = pd.Series(expReturns, index=idx)\n",
    "    return expReturns\n",
    "\n",
    "\n",
    "def zscore(a, axis=0, ddof=0, keepNaN=False):\n",
    "    \"\"\" NAN Stable Z-Scores\"\"\"\n",
    "    ##### JPM Oct 10 2017, Berowne Hlavaty & Robert Smith #####\n",
    "    try:\n",
    "        idx = a.index\n",
    "    except:\n",
    "        idx = None\n",
    "    a = np.asanyarray(a)\n",
    "    mns = np.nanmean(a, axis=axis)\n",
    "    sstd = np.nanstd(a=a, axis=axis, ddof=ddof)\n",
    "    if axis and mns.ndim < a.ndim:\n",
    "        res = ((a - np.expand_dims(mns, axis=axis)) /\n",
    "               np.expand_dims(sstd, axis=axis))\n",
    "    else:\n",
    "        res = (a - mns) / sstd # RESult\n",
    "    if not keepNaN:\n",
    "        res = np.nan_to_num(res) # Default set to zero where was NaN\n",
    "    res = pd.Series(res, index=idx)\n",
    "    return res\n",
    "\n",
    "\n",
    "def correlDist(corr):\n",
    "    # A distance matrix based on correlation, where 0<=d[i,j]<=1\n",
    "    # This is a proper distance metric\n",
    "    return ((1-corr) / 2.) ** .5\n",
    "\n",
    "\n",
    "def plotCorrMatrix(path, corr, labels=None):\n",
    "    # Heatmap of the correlation matrix\n",
    "    if labels is None:\n",
    "        labels = []\n",
    "    plt.pcolor(corr)\n",
    "    plt.colorbar()\n",
    "    plt.yticks(np.arange(.5, corr.shape[0] + .5), labels)\n",
    "    plt.xticks(np.arange(.5, corr.shape[0] + .5), labels)\n",
    "    plt.savefig(path)\n",
    "    plt.clf()\n",
    "    plt.close() # reset pylab\n",
    "    return\n",
    "\n",
    "\n",
    "def generateData(nobs, size0, size1, sigma1):\n",
    "    # Time series of correlated variables\n",
    "    # 1) generating some uncorrelated data\n",
    "    np.random.seed(seed=12345)\n",
    "    random.seed(12345)\n",
    "    x = np.random.normal(0, 1, size=(nobs,size0)) # each row is a variable\n",
    "    # 2) creating correlation between the variables\n",
    "    cols = [random.randint(0, size0-1) for i in range(size1)]\n",
    "    y = x[:,cols] + np.random.normal(0, sigma1, size=(nobs,len(cols)))\n",
    "    x = np.append(x,y,axis=1)\n",
    "    x = pd.DataFrame(x, columns=range(1, x.shape[1]+1))\n",
    "    return x, cols\n",
    "\n",
    "\n",
    "def getAllocWeights(rtns, exp_rtns=None, riskaversion=0.5):\n",
    "    rtns.fillna(0)\n",
    "    if exp_rtns is not None:\n",
    "        exp_rtns.fillna(0)\n",
    "    else:\n",
    "        exp_rtns = rtns.mean()\n",
    "    cov = rtns.cov()\n",
    "    corr = rtns.corr()\n",
    "    dist = correlDist(corr)\n",
    "    link = sch.linkage(dist, 'average') #'single') #https://towardsdatascience.com/introduction-hierarchical-clustering-d3066c6b560e\n",
    "    sortIx = getQuasiDiag(link)\n",
    "    sortIx = corr.index[sortIx].tolist() # recover labels\n",
    "    ivp = getIVP(cov)\n",
    "    hrp = getRecBipart(cov, sortIx)\n",
    "    harp = getHARP(cov, sortIx, exp_rtns, riskaversion)\n",
    "    hcp = getClusters(corr)\n",
    "    return ivp, hrp, harp, hcp\n",
    "\n",
    "\n",
    "def calcAllocWgts(rtns, lookback=260, numRun=0, exp_rtns=None, riskaversion=0.5, naIgnore=True):\n",
    "    if naIgnore:\n",
    "        X = rtns\n",
    "        if exp_rtns is not None:\n",
    "            EX = exp_rtns\n",
    "    else:\n",
    "        X = rtns.dropna(axis=1)\n",
    "        if exp_rtns is not None:\n",
    "            EX = exp_rtns[X.columns]\n",
    "            EX.fillna(method=\"ffill\", inplace=True)\n",
    "            EX.fillna(method=\"bfill\", inplace=True)\n",
    "            EX.fillna(value=0, inplace=True)\n",
    "    if numRun <= 0:\n",
    "        T = len(X.index)\n",
    "    else:\n",
    "        T = lookback + numRun + 1\n",
    "        X = X[-T:]\n",
    "        if exp_rtns is not None:\n",
    "            EX = EX[-T:]\n",
    "    dfIVP = np.zeros((T,len(X.columns)))\n",
    "    dfHRP = np.zeros((T,len(X.columns)))\n",
    "    dfHARP = np.zeros((T,len(X.columns)))\n",
    "    dfHCP = np.zeros((T,len(X.columns)))\n",
    "    for t in range(T):\n",
    "        if t > lookback:\n",
    "            if exp_rtns is not None:\n",
    "                ivp_wgt, hrp_wgt, harp_wgt, hcp_wgt = getAllocWeights(X[t-lookback+1:t], EX[t-1:t], riskaversion)\n",
    "            else:\n",
    "                ivp_wgt, hrp_wgt, harp_wgt, hcp_wgt = getAllocWeights(X[t-lookback+1:t])\n",
    "            dfIVP[t] = ivp_wgt\n",
    "            dfHRP[t] = hrp_wgt\n",
    "            dfHARP[t] = harp_wgt\n",
    "            dfHCP[t] = hcp_wgt\n",
    "    dfIVP = pd.DataFrame(dfIVP,index=X.index,columns=X.columns)\n",
    "    dfHRP = pd.DataFrame(dfHRP,index=X.index,columns=X.columns)\n",
    "    dfHARP = pd.DataFrame(dfHARP,index=X.index,columns=X.columns)\n",
    "    dfHCP = pd.DataFrame(dfHCP,index=X.index,columns=X.columns)\n",
    "    if not naIgnore:\n",
    "        dfIVP = dfIVP.reindex(columns=rtns.columns)\n",
    "        dfHRP = dfIVP.reindex(columns=rtns.columns)\n",
    "        dfHARP = dfIVP.reindex(columns=rtns.columns)\n",
    "        dfHCP = dfIVP.reindex(columns=rtns.columns)\n",
    "    return dfIVP, dfHRP, dfHARP, dfHCP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rp = pd.read_csv('cross_asset_allocation_plus_traditional.csv', \n",
    "                    index_col=['DATE'], parse_dates=['DATE'])\n",
    "df_ret = df_rp.pct_change(periods=1,fill_method=None)\n",
    "df_ret.columns = list(map_rp.keys())\n",
    "df_ret = df_ret[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.024307662798539797, 0.04331681708075773, 0.016361859887551667, 0.09106708544039407, 0.030971600365334126, 0.10285574699873355, 0.035499096773310736, 0.1163887277361239, 0.3481195323620805, 0.005290719556838739, 0.005837245403893006, 0.003571429316128669, 0.0032759291885260705, 0.09240981154876056, 0.015809372164622845, 0.02991313717082113, 0.0010544205374466151, 0.017041957665176088, 0.0072709698005083145, 0.009636878204452203]\n",
      "[0.0625, 0.0625, 0.03125, 0.03125, 0.0625, 0.0625, 0.015625, 0.015625, 0.03125, 0.125, 0.03125, 0.03125, 0.015625, 0.0625, 0.0078125, 0.03125, 0.0625, 0.125, 0.0078125, 0.125]\n",
      "[0.0017968650804614703, 0.0019824793810478748, 0.001214230369820565, 0.0006517310160058874, 0.0019172125103651772, 0.005008011931733589, 0.0021366737455654856, 0.005783412117137195, 0.014153764803272646, 0.011501466402641047, 0.017063848032767233, 0.0949742215995985, 0.0013387747657244362, 0.35355669480416974, 0.036053545282402656, 0.13987379272475842, 0.05205733932917342, 0.039072847103024264, 0.11581244754475022, 0.10405064145558014]\n",
      "[0.005958627886397459, 0.04143532301000159, 0.02839640995394469, 0.010856208632602086, 0.018611925280467366, 0.04960930628106331, 0.03836028273380224, 0.06057377155104341, 0.04998202517959723, 0.038336886707483496, 0.02758776127433028, 0.07642871729713135, 0.12409095173246444, 0.11871313957496765, 0.06525546063267385, 0.08639986519643934, 0.02402050052270121, 0.011016813641954457, 0.052003596166942605, 0.072362426743992]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tqiu\\AppData\\Local\\Temp\\ipykernel_32712\\1624920376.py:237: ClusterWarning: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  link = sch.linkage(dist, 'average') #'single') #https://towardsdatascience.com/introduction-hierarchical-clustering-d3066c6b560e\n",
      "C:\\Users\\tqiu\\AppData\\Local\\Temp\\ipykernel_32712\\1624920376.py:69: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.04614585 0.04614585 0.04614585 0.04614585 0.04614585 0.04614585\n",
      " 0.04614585 0.04614585 0.04614585 0.04614585]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  w[cItems0] *= alpha # weight 1\n"
     ]
    }
   ],
   "source": [
    "df_ivp, df_hrp, df_harp, df_hcp = getAllocWeights(df_ret)\n",
    "print(list(df_ivp))\n",
    "print(list(df_hcp))\n",
    "print(list(df_hrp))\n",
    "print(list(df_harp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12947649931675584, 0.2307301151757658, 0.08715261348280258, 0.48507532474534887, 0.031092503907627446, 0.01902347293699308, 0.01744947043470654]\n",
      "[0.125, 0.25, 0.0625, 0.0625, 0.125, 0.125, 0.25]\n",
      "[0.013453220809716952, 0.012712542679853835, 0.007777974792542936, 0.2544290632190139, 0.14277539975631026, 0.08663841387807396, 0.4822133848644882]\n",
      "[0.14346802382184334, 0.14715606847075008, 0.10971749886359215, 0.1014935781091766, 0.16411580437225576, 0.08859812329520936, 0.2454509030671727]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tqiu\\AppData\\Local\\Temp\\ipykernel_32712\\1624920376.py:237: ClusterWarning: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  link = sch.linkage(dist, 'average') #'single') #https://towardsdatascience.com/introduction-hierarchical-clustering-d3066c6b560e\n",
      "C:\\Users\\tqiu\\AppData\\Local\\Temp\\ipykernel_32712\\1624920376.py:69: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.03394374 0.03394374 0.03394374]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  w[cItems0] *= alpha # weight 1\n"
     ]
    }
   ],
   "source": [
    "EQ_Idx = [0,1,2,3,10,11,12]\n",
    "df_ivp_eq, df_hrp_eq, df_harp_eq, df_hcp_eq = getAllocWeights(df_ret.iloc[:,EQ_Idx])\n",
    "print(list(df_ivp_eq))\n",
    "print(list(df_hcp_eq))\n",
    "print(list(df_hrp_eq))\n",
    "print(list(df_harp_eq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03813006329918311, 0.12662878564506375, 0.043703999504823864, 0.14328964288384186, 0.4285803654984927, 0.006513563045527524, 0.11376848216612358, 0.019463390791782526, 0.03682695761105052, 0.001298128652137681, 0.020980863657355044, 0.008951508332457456, 0.01186424891216035]\n",
      "[0.0625, 0.0625, 0.0625, 0.0625, 0.125, 0.125, 0.125, 0.015625, 0.03125, 0.25, 0.03125, 0.015625, 0.03125]\n",
      "[0.001605724685919088, 0.42405543281346153, 0.043242574596578334, 0.22891154049649884, 0.05221985769257532, 0.008920450318695167, 0.013416956735037778, 0.01846041423185821, 0.005947245270204813, 0.01017639033906827, 0.004341768022622618, 0.09939821840528862, 0.08930342639219144]\n",
      "[0.1765695223891541, 0.16891741149073014, 0.09285226163819542, 0.13681614642651294, 0.0, 0.0, 0.07192426269432647, 0.04978546830399817, 0.02742510049348915, 0.06161164794099855, 0.04764106599923073, 0.06960396627123815, 0.09685314635212616]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tqiu\\AppData\\Local\\Temp\\ipykernel_32712\\1624920376.py:237: ClusterWarning: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
      "  link = sch.linkage(dist, 'average') #'single') #https://towardsdatascience.com/introduction-hierarchical-clustering-d3066c6b560e\n",
      "C:\\Users\\tqiu\\AppData\\Local\\Temp\\ipykernel_32712\\1624920376.py:69: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[0.75895558 0.75895558 0.75895558 0.75895558 0.75895558 0.75895558]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  w[cItems0] *= alpha # weight 1\n"
     ]
    }
   ],
   "source": [
    "FICC_Idx = [4,5,6,7,8,9,13,14,15,16,17,18,19]\n",
    "df_ivp_ficc, df_hrp_ficc, df_harp_ficc, df_hcp_ficc = getAllocWeights(df_ret.iloc[:,FICC_Idx])\n",
    "print(list(df_ivp_ficc))\n",
    "print(list(df_hcp_ficc))\n",
    "print(list(df_hrp_ficc))\n",
    "print(list(df_harp_ficc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
